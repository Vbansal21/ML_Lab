{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1925d1ba-f8bb-4510-82cc-962e4da66472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T05:36:44.939822Z",
     "iopub.status.busy": "2025-02-20T05:36:44.938832Z",
     "iopub.status.idle": "2025-02-20T05:36:53.569990Z",
     "shell.execute_reply": "2025-02-20T05:36:53.568381Z",
     "shell.execute_reply.started": "2025-02-20T05:36:44.939766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
      "Dataset downloaded successfully! an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
      "\n",
      "Training Accuracy:  0.681sfully! an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
      "\n",
      "\n",
      "Testing Accuracy:   0.683\n",
      "Training Precision: 0.390\n",
      "Testing Precision:  0.396\n",
      "Training Recall:    0.728\n",
      "Testing Recall:     0.728\n",
      "Training F1:        0.508\n",
      "Testing F1:         0.513\n",
      "\n",
      "Confusion Matrix (Test):\n",
      "[[17623  8685]\n",
      " [ 2124  5699]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1) Download & Load Spotify Dataset\n",
    "# -----------------------------------------------------\n",
    "def download_data():\n",
    "    print(\"Downloading dataset...\")\n",
    "    path = kagglehub.dataset_download(\"yamaerenay/spotify-dataset-1921-2020-160k-tracks\")\n",
    "    if not os.path.exists('data.csv'):\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            if 'data.csv' in files:\n",
    "                shutil.copy2(os.path.join(root, 'data.csv'), 'data.csv')\n",
    "                break\n",
    "    if os.path.exists('data.csv'):\n",
    "        print(\"Dataset downloaded successfully!\")\n",
    "        return pd.read_csv('data.csv')\n",
    "    else:\n",
    "        raise FileNotFoundError(\"data.csv not found after download.\")\n",
    "\n",
    "df = download_data()\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 2) Data Preparation\n",
    "# -----------------------------------------------------\n",
    "# Choose numeric features\n",
    "features = [\n",
    "    'acousticness', 'danceability', 'duration_ms', 'energy',\n",
    "    'instrumentalness', 'liveness', 'loudness', 'mode',\n",
    "    'speechiness', 'tempo', 'valence'\n",
    "]\n",
    "\n",
    "# Drop rows with missing values in features or popularity\n",
    "df = df.dropna(subset=features + ['popularity'])\n",
    "\n",
    "X = df[features].values\n",
    "y_regression = df['popularity'].values\n",
    "\n",
    "# Convert popularity into a binary classification (you can adjust the threshold)\n",
    "threshold = 50\n",
    "y = (y_regression >= threshold).astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 3) Gaussian Naive Bayes (from scratch)\n",
    "# -----------------------------------------------------\n",
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Implements a simple Gaussian Naive Bayes classifier for binary classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        - Compute mean, variance, and prior probabilities for each class.\n",
    "        \"\"\"\n",
    "        # Separate data by class\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.mean_ = {}\n",
    "        self.var_ = {}\n",
    "        self.prior_ = {}\n",
    "\n",
    "        for cls in self.classes_:\n",
    "            X_c = X[y == cls]\n",
    "            self.mean_[cls] = X_c.mean(axis=0)\n",
    "            self.var_[cls]  = X_c.var(axis=0) + 1e-9  # add small value to avoid division by zero\n",
    "            self.prior_[cls] = len(X_c) / len(X)\n",
    "    \n",
    "    def _gaussian_pdf(self, X, cls):\n",
    "        \"\"\"\n",
    "        Compute the Gaussian probability density function for each feature\n",
    "        given class 'cls'.\n",
    "        p(x_i | cls) = (1 / sqrt(2*pi*var)) * exp(-(x_i - mean)^2 / (2*var))\n",
    "        Returns vector of likelihood for each sample.\n",
    "        \"\"\"\n",
    "        mean = self.mean_[cls]\n",
    "        var  = self.var_[cls]\n",
    "        \n",
    "        # For each feature:\n",
    "        numerator = -0.5 * ((X - mean)**2 / var)\n",
    "        exponent  = np.exp(numerator)\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        # Combine across features\n",
    "        # shape: [num_samples, num_features]\n",
    "        likelihood = exponent / denominator\n",
    "        # Multiply feature likelihoods => shape [num_samples]\n",
    "        # (naive assumption: features are independent)\n",
    "        return np.prod(likelihood, axis=1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class for each sample by picking the class with the largest posterior.\n",
    "        posterior ~ prior * p(X|cls)\n",
    "        \"\"\"\n",
    "        posteriors = []\n",
    "        for cls in self.classes_:\n",
    "            prior = self.prior_[cls]\n",
    "            class_cond_likelihood = self._gaussian_pdf(X, cls)\n",
    "            posterior = prior * class_cond_likelihood\n",
    "            posteriors.append(posterior.reshape(-1, 1))\n",
    "        \n",
    "        # posteriors: list of [num_samples x 1], one for each class\n",
    "        # We combine them side-by-side and pick argmax\n",
    "        posteriors = np.hstack(posteriors)  # shape [num_samples, n_classes]\n",
    "        # Argmax across columns\n",
    "        class_index = np.argmax(posteriors, axis=1)\n",
    "        predictions = self.classes_[class_index]\n",
    "        return predictions\n",
    "\n",
    "# Instantiate and train\n",
    "gnb = GaussianNaiveBayes()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Inference\n",
    "y_pred_train = gnb.predict(X_train_scaled)\n",
    "y_pred_test  = gnb.predict(X_test_scaled)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4) Evaluation\n",
    "# -----------------------------------------------------\n",
    "train_accuracy  = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy   = accuracy_score(y_test,  y_pred_test)\n",
    "train_precision = precision_score(y_train, y_pred_train, average='binary')\n",
    "test_precision  = precision_score(y_test,  y_pred_test,  average='binary')\n",
    "train_recall    = recall_score(y_train, y_pred_train, average='binary')\n",
    "test_recall     = recall_score(y_test,  y_pred_test,  average='binary')\n",
    "train_f1        = f1_score(y_train, y_pred_train, average='binary')\n",
    "test_f1         = f1_score(y_test,  y_pred_test,  average='binary')\n",
    "\n",
    "print(f\"Training Accuracy:  {train_accuracy:.3f}\")\n",
    "print(f\"Testing Accuracy:   {test_accuracy:.3f}\")\n",
    "print(f\"Training Precision: {train_precision:.3f}\")\n",
    "print(f\"Testing Precision:  {test_precision:.3f}\")\n",
    "print(f\"Training Recall:    {train_recall:.3f}\")\n",
    "print(f\"Testing Recall:     {test_recall:.3f}\")\n",
    "print(f\"Training F1:        {train_f1:.3f}\")\n",
    "print(f\"Testing F1:         {test_f1:.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"\\nConfusion Matrix (Test):\")\n",
    "print(cm)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 5) Visualize Confusion Matrix\n",
    "# -----------------------------------------------------\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Naive Bayes Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(gnb.classes_))\n",
    "plt.xticks(tick_marks, gnb.classes_)\n",
    "plt.yticks(tick_marks, gnb.classes_)\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j]),\n",
    "                 ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a9eefb-7315-448f-a816-6fde50ddec8f",
   "metadata": {},
   "source": [
    "# Section 1: Understanding Our Spotify Dataset 🎵\n",
    "\n",
    "Before we dive into building our Naive Bayes classifier, it's crucial to understand our data and establish our project's foundation. In this section, we'll set up our environment and perform initial data exploration.\n",
    "\n",
    "The Spotify dataset contains audio features for songs, including characteristics like tempo, energy, and danceability. Our goal is to predict whether a song will be popular based on these musical attributes. This is an excellent use case for Naive Bayes classification because:\n",
    "\n",
    "1. The features are naturally continuous and often follow approximate normal distributions\n",
    "2. While not entirely independent, the musical features have reasonable independence (e.g., tempo doesn't completely determine energy)\n",
    "3. We have a clear binary classification task (popular vs. not popular)\n",
    "\n",
    "Let's start by importing our dependencies and loading our data. We'll also include some initial exploration to understand our dataset's characteristics. Pay special attention to the data types and distributions - this understanding will inform our preprocessing decisions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b6b4ac8-7076-408f-b7cf-f63c9b0d93c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T06:02:45.062557Z",
     "iopub.status.busy": "2025-02-20T06:02:45.060979Z",
     "iopub.status.idle": "2025-02-20T06:02:45.140075Z",
     "shell.execute_reply": "2025-02-20T06:02:45.138664Z",
     "shell.execute_reply.started": "2025-02-20T06:02:45.062523Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.10/site-packages/matplotlib/style/core.py:129\u001b[0m, in \u001b[0;36muse\u001b[0;34m(style)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     style \u001b[38;5;241m=\u001b[39m \u001b[43m_rc_params_in_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.10/site-packages/matplotlib/__init__.py:903\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[0;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[1;32m    902\u001b[0m rc_temp \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_or_url(fname) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.10/site-packages/matplotlib/__init__.py:880\u001b[0m, in \u001b[0;36m_open_file_or_url\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    879\u001b[0m fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname)\n\u001b[0;32m--> 880\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seaborn'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Set up plotting style for better visualization\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseaborn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_palette(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhusl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.10/site-packages/matplotlib/style/core.py:131\u001b[0m, in \u001b[0;36muse\u001b[0;34m(style)\u001b[0m\n\u001b[1;32m    129\u001b[0m         style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid package style, path of style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, URL of style file, or library style name (library \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyles are listed in `style.available`)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    135\u001b[0m filtered \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: 'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set up plotting style for better visualization\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the dataset\n",
    "def load_and_explore_data(filepath='spotify_data.csv'):\n",
    "    \"\"\"\n",
    "    Load and perform initial exploration of the Spotify dataset.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the dataset\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataframe with initial insights\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"Dataset Overview:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Number of songs: {len(df):,}\")\n",
    "    print(f\"Number of features: {df.shape[1]}\")\n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate and display feature statistics\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    stats_df = df[numeric_columns].agg(['mean', 'std', 'min', 'max']).round(2)\n",
    "    print(stats_df)\n",
    "    \n",
    "    # Visualize popularity distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df, x='popularity', bins=50)\n",
    "    plt.title('Distribution of Song Popularity')\n",
    "    plt.xlabel('Popularity Score')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Add vertical line at our classification threshold\n",
    "    plt.axvline(x=50, color='red', linestyle='--', \n",
    "                label='Classification Threshold (50)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Test for normality in key features\n",
    "    print(\"\\nNormality Tests for Key Features:\")\n",
    "    print(\"-\" * 50)\n",
    "    for feature in ['danceability', 'energy', 'tempo']:\n",
    "        statistic, p_value = stats.normaltest(df[feature])\n",
    "        print(f\"{feature:12} - p-value: {p_value:.4f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute the exploration\n",
    "df = load_and_explore_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68580c1e-12b5-455f-8804-11bc40a4f280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T06:05:26.720716Z",
     "iopub.status.busy": "2025-02-20T06:05:26.720118Z",
     "iopub.status.idle": "2025-02-20T06:05:33.349141Z",
     "shell.execute_reply": "2025-02-20T06:05:33.344244Z",
     "shell.execute_reply.started": "2025-02-20T06:05:26.720657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (25.0.1)\n",
      "\u001b[33mWARNING: Error parsing dependencies of huggingface-hub: [Errno 2] No such file or directory: '/home/user/miniforge3/envs/myenv/lib/python3.10/site-packages/huggingface_hub-0.27.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Error parsing dependencies of transformers: [Errno 2] No such file or directory: '/home/user/miniforge3/envs/myenv/lib/python3.10/site-packages/transformers-4.48.1.dist-info/METADATA'\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "Requirement already satisfied: seaborn in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (0.13.2)orge3/envs/myenv/lib/python3.10/site-packages/transformers-4.48.1.dist-info/METADATA'\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (3.10.0)nvs/myenv/lib/python3.10/site-packages/transformers-4.48.1.dist-info/METADATA'\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\n",
      "transformers in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (4.48.1)s/myenv/lib/python3.10/site-packages/transformers-4.48.1.dist-info/METADATA'\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "Collecting transformersser/miniforge3/envs/myenv/lib/python3.10/site-packages (4.48.1)s/myenv/lib/python3.10/site-packages/transformers-4.48.1.dist-info/METADATA'\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from seaborn) (2.2.2)8.1.dist-info/METADATA'\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "\n",
      "Requirement already satisfied: pandas>=1.2 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "contourpy>=1.0.1 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from matplotlib) (1.3.1))8.1.dist-info/METADATA'\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "\n",
      "Requirement already satisfied: pandas>=1.2 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user/miniforge3/envs/myenv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/home/user/miniforge3/envs/myenv/lib/python3.10/site-packages/huggingface_hub-0.27.1.dist-info/METADATA'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install --force-reinstall seaborn matplotlib transformers huggingface_hub --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5f5d2-b7b5-4c96-87f0-3451952a9d76",
   "metadata": {},
   "source": [
    "This initial code block does several important things:\n",
    "\n",
    "1. We set up our environment with necessary libraries and consistent styling\n",
    "2. We create a comprehensive data loading function that not only reads the data but provides immediate insights\n",
    "3. We visualize the popularity distribution to understand our classification threshold\n",
    "4. We perform normality tests on key features (important for Naive Bayes assumptions)\n",
    "\n",
    "The visualization of popularity distribution helps us understand why we chose 50 as our threshold - it typically represents a natural midpoint in the popularity scores. The normality tests give us insight into whether our features follow Gaussian distributions, which is an assumption of Gaussian Naive Bayes.\n",
    "\n",
    "In our next section, we'll begin preprocessing this data, but first, would you like to explore any particular aspect of the dataset more deeply? For example, we could:\n",
    "- Analyze correlations between specific features\n",
    "- Investigate the relationship between certain features and popularity\n",
    "- Look for potential outliers or anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad5168-e0d8-46a2-85b7-0a9ed6ac1cb2",
   "metadata": {},
   "source": [
    "# Section 2: Preparing Our Data for Naive Bayes Classification 🔧\n",
    "\n",
    "Data preprocessing is particularly important for Naive Bayes classification because the algorithm makes specific assumptions about our data. The Gaussian Naive Bayes classifier assumes that our features follow a normal distribution within each class. Additionally, while the algorithm is called \"naive\" because it assumes feature independence, we can still improve its performance by carefully selecting and transforming our features.\n",
    "\n",
    "In this section, we'll prepare our data through several important steps. We'll handle missing values, scale our features, and engineer new features that might help capture important patterns in our music data. We'll also examine the impact of each preprocessing step on our data's characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b0c80-863b-458d-aab8-8dcff23ccc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "class SpotifyDataPreprocessor:\n",
    "    \"\"\"\n",
    "    A comprehensive preprocessor for Spotify track data.\n",
    "    This class handles all necessary transformations to prepare\n",
    "    data for Naive Bayes classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, popularity_threshold=50):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor with configuration parameters.\n",
    "        \n",
    "        Args:\n",
    "            popularity_threshold (int): Threshold for binary classification\n",
    "        \"\"\"\n",
    "        self.popularity_threshold = popularity_threshold\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = [\n",
    "            'acousticness', 'danceability', 'duration_ms', 'energy',\n",
    "            'instrumentalness', 'liveness', 'loudness', 'speechiness',\n",
    "            'tempo', 'valence'\n",
    "        ]\n",
    "    \n",
    "    def _handle_missing_values(self, df):\n",
    "        \"\"\"\n",
    "        Handle missing values in the dataset.\n",
    "        We'll use median imputation for numeric features.\n",
    "        \"\"\"\n",
    "        print(\"Missing values before handling:\")\n",
    "        print(df[self.feature_names].isnull().sum())\n",
    "        \n",
    "        # Impute missing values with median\n",
    "        for feature in self.feature_names:\n",
    "            median_value = df[feature].median()\n",
    "            df[feature].fillna(median_value, inplace=True)\n",
    "        \n",
    "        print(\"\\nMissing values after handling:\")\n",
    "        print(df[self.feature_names].isnull().sum())\n",
    "        return df\n",
    "    \n",
    "    def _create_interaction_features(self, df):\n",
    "        \"\"\"\n",
    "        Create meaningful feature interactions.\n",
    "        For example, energy * tempo might capture \"intensity\"\n",
    "        \"\"\"\n",
    "        df['intensity'] = df['energy'] * df['tempo'] / 100\n",
    "        df['melodic_complexity'] = df['instrumentalness'] * (1 - df['speechiness'])\n",
    "        self.feature_names.extend(['intensity', 'melodic_complexity'])\n",
    "        return df\n",
    "    \n",
    "    def _visualize_distributions(self, df, features):\n",
    "        \"\"\"\n",
    "        Visualize the distribution of each feature by popularity class.\n",
    "        \"\"\"\n",
    "        n_features = len(features)\n",
    "        n_cols = 3\n",
    "        n_rows = (n_features + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, feature in enumerate(features):\n",
    "            sns.kdeplot(\n",
    "                data=df,\n",
    "                x=feature,\n",
    "                hue='is_popular',\n",
    "                ax=axes[idx]\n",
    "            )\n",
    "            axes[idx].set_title(f'{feature} Distribution by Popularity')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for idx in range(n_features, len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"\n",
    "        Preprocess the data and return features ready for Naive Bayes.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Raw Spotify track data\n",
    "            \n",
    "        Returns:\n",
    "            tuple: X (features) and y (target) arrays\n",
    "        \"\"\"\n",
    "        print(\"Starting preprocessing pipeline...\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = self._handle_missing_values(df)\n",
    "        \n",
    "        # Create binary target\n",
    "        df['is_popular'] = (df['popularity'] >= self.popularity_threshold).astype(int)\n",
    "        \n",
    "        # Create interaction features\n",
    "        df = self._create_interaction_features(df)\n",
    "        \n",
    "        # Scale features\n",
    "        X = self.scaler.fit_transform(df[self.feature_names])\n",
    "        \n",
    "        # Visualize distributions\n",
    "        print(\"\\nVisualizing feature distributions...\")\n",
    "        self._visualize_distributions(df, self.feature_names)\n",
    "        \n",
    "        # Check normality of scaled features\n",
    "        print(\"\\nNormality tests for scaled features:\")\n",
    "        for idx, feature in enumerate(self.feature_names):\n",
    "            _, p_value = stats.normaltest(X[:, idx])\n",
    "            print(f\"{feature:20} p-value: {p_value:.4f}\")\n",
    "        \n",
    "        return X, df['is_popular'].values\n",
    "\n",
    "# Usage example\n",
    "preprocessor = SpotifyDataPreprocessor()\n",
    "X, y = preprocessor.fit_transform(df)\n",
    "\n",
    "print(\"\\nFinal dataset shape:\", X.shape)\n",
    "print(\"Popularity class distribution:\")\n",
    "print(pd.Series(y).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3035ac-2299-4bb6-82ad-1832b18611f6",
   "metadata": {},
   "source": [
    "Let's examine why each step in our preprocessing pipeline is important:\n",
    "\n",
    "1. **Missing Value Handling**: We use median imputation because it's robust to outliers and preserves the feature's distribution. This is particularly important for Naive Bayes, which relies on these distributions for its probability calculations.\n",
    "\n",
    "2. **Feature Interaction Creation**: We create new features like 'intensity' and 'melodic_complexity' because they might capture meaningful musical patterns that individual features miss. While this technically violates the naive independence assumption, in practice it often improves model performance.\n",
    "\n",
    "3. **Feature Scaling**: Although Naive Bayes can work with unscaled features (since it models each feature's distribution separately), scaling helps us:\n",
    "   - Compare feature importances more easily\n",
    "   - Visualize distributions more effectively\n",
    "   - Create meaningful interaction features\n",
    "\n",
    "4. **Distribution Visualization**: We visualize the distributions to verify that our features approximately follow normal distributions within each class - a key assumption of Gaussian Naive Bayes.\n",
    "\n",
    "By examining the normality test results and visualizations, we can identify which features might need additional transformation or might be better modeled using a different variant of Naive Bayes (like Multinomial for discrete features).\n",
    "\n",
    "The preprocessing steps we've implemented help ensure our data meets the assumptions of Naive Bayes while maximizing the information available for classification. In the next section, we'll implement the Naive Bayes classifier itself and see how it performs with our carefully prepared data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55d3a6-61b6-4214-a4a8-cf4d4163a326",
   "metadata": {},
   "source": [
    "# Section 3: The Mathematics and Implementation of Naive Bayes 📐\n",
    "\n",
    "In this section, we'll explore the mathematical foundation of Naive Bayes classification and implement these concepts in code. The beauty of Naive Bayes lies in its simplicity and the clear connection between its mathematical principles and practical implementation.\n",
    "\n",
    "Let's start by understanding what makes Naive Bayes \"naive\" and why it works well for our music classification task. Imagine you're trying to determine if a song will be popular. You might consider various features like tempo, energy, and danceability. Naive Bayes makes the simplifying assumption that these features contribute independently to a song's popularity. While this isn't entirely true (fast tempo often correlates with high energy), this \"naive\" assumption often works surprisingly well in practice.\n",
    "\n",
    "The core of Naive Bayes is Bayes' Theorem:\n",
    "\n",
    "$P(Popular|Features) = P(Features|Popular) * P(Popular) / P(Features)$\n",
    "\n",
    "For our Spotify data, we're using Gaussian Naive Bayes because our features are continuous variables. Let's implement this mathematical framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192f867-07fc-4d03-82bc-6f8bcfe13c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class GaussianNaiveBayesClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    A detailed implementation of Gaussian Naive Bayes classifier that exposes\n",
    "    the underlying probability calculations for educational purposes.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}      # P(Class)\n",
    "        self.means = {}             # μ for each feature in each class\n",
    "        self.variances = {}         # σ² for each feature in each class\n",
    "        self.feature_distributions = {}  # Store distribution objects for visualization\n",
    "        \n",
    "    def _calculate_gaussian_probability(self, x, mean, variance):\n",
    "        \"\"\"\n",
    "        Calculate the Gaussian probability density function.\n",
    "        \n",
    "        Args:\n",
    "            x (float): Feature value\n",
    "            mean (float): Mean of the feature for a class\n",
    "            variance (float): Variance of the feature for a class\n",
    "            \n",
    "        Returns:\n",
    "            float: Probability density for the feature value\n",
    "        \"\"\"\n",
    "        # Adding small epsilon to variance to prevent division by zero\n",
    "        epsilon = 1e-10\n",
    "        exponent = -0.5 * ((x - mean) ** 2) / (variance + epsilon)\n",
    "        coefficient = 1 / np.sqrt(2 * np.pi * (variance + epsilon))\n",
    "        return coefficient * np.exp(exponent)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the classifier by calculating means and variances for each feature\n",
    "        within each class.\n",
    "        \n",
    "        Args:\n",
    "            X (np.array): Features matrix of shape (n_samples, n_features)\n",
    "            y (np.array): Target array of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Calculate class priors\n",
    "        for class_label in self.classes:\n",
    "            class_samples = y == class_label\n",
    "            self.class_priors[class_label] = np.sum(class_samples) / n_samples\n",
    "            \n",
    "            # Calculate mean and variance for each feature in this class\n",
    "            class_data = X[class_samples]\n",
    "            self.means[class_label] = np.mean(class_data, axis=0)\n",
    "            self.variances[class_label] = np.var(class_data, axis=0)\n",
    "            \n",
    "            # Store distribution objects for later visualization\n",
    "            self.feature_distributions[class_label] = []\n",
    "            for feature_idx in range(X.shape[1]):\n",
    "                dist = norm(self.means[class_label][feature_idx], \n",
    "                          np.sqrt(self.variances[class_label][feature_idx]))\n",
    "                self.feature_distributions[class_label].append(dist)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _calculate_class_probability(self, x, class_label):\n",
    "        \"\"\"\n",
    "        Calculate P(Features|Class) * P(Class) for a single instance.\n",
    "        \n",
    "        Args:\n",
    "            x (np.array): Single instance features\n",
    "            class_label: Class label to calculate probability for\n",
    "            \n",
    "        Returns:\n",
    "            float: Log probability for the instance belonging to the class\n",
    "        \"\"\"\n",
    "        # Use log probabilities to prevent numerical underflow\n",
    "        log_prob = np.log(self.class_priors[class_label])\n",
    "        \n",
    "        for feature_idx, feature_value in enumerate(x):\n",
    "            probability = self._calculate_gaussian_probability(\n",
    "                feature_value,\n",
    "                self.means[class_label][feature_idx],\n",
    "                self.variances[class_label][feature_idx]\n",
    "            )\n",
    "            # Add small epsilon to prevent log(0)\n",
    "            log_prob += np.log(probability + 1e-10)\n",
    "            \n",
    "        return log_prob\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Calculate probability estimates for each class.\n",
    "        \n",
    "        Args:\n",
    "            X (np.array): Features matrix\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Probability estimates for each class\n",
    "        \"\"\"\n",
    "        probabilities = np.zeros((X.shape[0], len(self.classes)))\n",
    "        \n",
    "        for i, x in enumerate(X):\n",
    "            class_probs = [self._calculate_class_probability(x, c) \n",
    "                         for c in self.classes]\n",
    "            # Convert from log probabilities and normalize\n",
    "            log_prob_sum = np.logaddexp.reduce(class_probs)\n",
    "            probabilities[i] = np.exp([p - log_prob_sum for p in class_probs])\n",
    "            \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Args:\n",
    "            X (np.array): Features matrix\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Predicted class labels\n",
    "        \"\"\"\n",
    "        return self.classes[np.argmax(self.predict_proba(X), axis=1)]\n",
    "    \n",
    "    def visualize_feature_distributions(self, feature_names):\n",
    "        \"\"\"\n",
    "        Visualize the learned Gaussian distributions for each feature.\n",
    "        \n",
    "        Args:\n",
    "            feature_names (list): Names of the features\n",
    "        \"\"\"\n",
    "        n_features = len(feature_names)\n",
    "        n_cols = 3\n",
    "        n_rows = (n_features + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for feature_idx, (feature_name, ax) in enumerate(zip(feature_names, axes)):\n",
    "            x = np.linspace(-4, 4, 1000)  # Standardized feature range\n",
    "            \n",
    "            for class_label in self.classes:\n",
    "                distribution = self.feature_distributions[class_label][feature_idx]\n",
    "                y = distribution.pdf(x)\n",
    "                ax.plot(x, y, label=f'Class {class_label}')\n",
    "                \n",
    "            ax.set_title(f'{feature_name} Distribution by Class')\n",
    "            ax.legend()\n",
    "            \n",
    "        # Hide empty subplots\n",
    "        for idx in range(n_features, len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage with our preprocessed data\n",
    "classifier = GaussianNaiveBayesClassifier()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# Visualize what the classifier learned\n",
    "classifier.visualize_feature_distributions(preprocessor.feature_names)\n",
    "\n",
    "# Make predictions\n",
    "predictions = classifier.predict(X)\n",
    "probabilities = classifier.predict_proba(X)\n",
    "\n",
    "print(\"Sample prediction probabilities:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i}: Popular: {probabilities[i][1]:.3f}, \"\n",
    "          f\"Not Popular: {probabilities[i][0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb2c038-5bc2-470d-823c-afb6b9aef0d6",
   "metadata": {},
   "source": [
    "Our implementation reveals several key insights about Naive Bayes:\n",
    "\n",
    "1. The classifier learns by calculating simple statistics (mean and variance) for each feature within each class. This makes it very efficient to train and easy to understand.\n",
    "\n",
    "2. We use log probabilities to prevent numerical underflow. When multiplying many small probabilities together, the result can become too small for computers to handle accurately. Taking the log transforms multiplication into addition and solves this problem.\n",
    "\n",
    "3. The \"naive\" independence assumption allows us to simply add the log probabilities of each feature. In reality, features might be correlated, but this simplification often works well in practice.\n",
    "\n",
    "4. For each prediction, we calculate class probabilities using Bayes' Theorem. The normalization step (dividing by the sum of probabilities) ensures our predictions sum to 1.\n",
    "\n",
    "The visualization of feature distributions helps us understand what the classifier has learned. Features with clear separation between class distributions will be more useful for prediction than those with heavily overlapping distributions.\n",
    "\n",
    "In the next section, we'll evaluate our classifier's performance and explore techniques to improve its accuracy. We'll also examine cases where the naive independence assumption might lead to incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4483e9-7f98-48d5-a0d9-757160490118",
   "metadata": {},
   "source": [
    "# Section 4: Evaluating Our Naive Bayes Classifier 📊\n",
    "\n",
    "When we evaluate a machine learning model, we're not just looking for a single accuracy number. Instead, we want to understand how our model performs across different situations and what kinds of mistakes it tends to make. This understanding helps us improve the model and know when we can trust its predictions.\n",
    "\n",
    "Think of our Spotify popularity classifier like a music critic. Just as we wouldn't trust a critic who only says \"good\" or \"bad\" without explanation, we need to understand the nuances of our model's decision-making process. Let's build a comprehensive evaluation system that helps us understand exactly how our classifier is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043817a2-9c64-49b5-8bc2-e2397c794be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    A comprehensive evaluation toolkit for our Naive Bayes classifier.\n",
    "    This class helps us understand model performance through multiple lenses.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, X, y, feature_names):\n",
    "        self.model = model\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "    def analyze_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Analyze how each feature contributes to classification decisions.\n",
    "        We do this by measuring the separation between class distributions\n",
    "        for each feature.\n",
    "        \"\"\"\n",
    "        feature_scores = []\n",
    "        \n",
    "        # For each feature, calculate the separation between classes\n",
    "        for i, feature in enumerate(self.feature_names):\n",
    "            class_0_mean = self.model.means[0][i]\n",
    "            class_1_mean = self.model.means[1][i]\n",
    "            pooled_std = np.sqrt((self.model.variances[0][i] + \n",
    "                                self.model.variances[1][i]) / 2)\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            effect_size = np.abs(class_1_mean - class_0_mean) / pooled_std\n",
    "            feature_scores.append((feature, effect_size))\n",
    "        \n",
    "        # Sort features by importance\n",
    "        feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        features, scores = zip(*feature_scores)\n",
    "        sns.barplot(x=list(scores), y=list(features))\n",
    "        plt.title('Feature Importance in Classification Decisions')\n",
    "        plt.xlabel('Effect Size (Class Separation)')\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_scores\n",
    "    \n",
    "    def analyze_decision_boundaries(self, feature1_idx, feature2_idx):\n",
    "        \"\"\"\n",
    "        Visualize decision boundaries for two selected features.\n",
    "        This helps us understand how the model makes decisions in\n",
    "        different regions of the feature space.\n",
    "        \"\"\"\n",
    "        # Create a mesh grid\n",
    "        x_min, x_max = self.X[:, feature1_idx].min() - 1, self.X[:, feature1_idx].max() + 1\n",
    "        y_min, y_max = self.X[:, feature2_idx].min() - 1, self.X[:, feature2_idx].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                            np.arange(y_min, y_max, 0.1))\n",
    "        \n",
    "        # Make predictions for each point in the mesh\n",
    "        X_mesh = np.zeros((xx.ravel().shape[0], self.X.shape[1]))\n",
    "        X_mesh[:, feature1_idx] = xx.ravel()\n",
    "        X_mesh[:, feature2_idx] = yy.ravel()\n",
    "        Z = self.model.predict(X_mesh)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        # Plot decision boundary and training points\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "        scatter = plt.scatter(self.X[:, feature1_idx], self.X[:, feature2_idx], \n",
    "                            c=self.y, alpha=0.8)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.xlabel(self.feature_names[feature1_idx])\n",
    "        plt.ylabel(self.feature_names[feature2_idx])\n",
    "        plt.title('Decision Boundaries and Training Data')\n",
    "        plt.show()\n",
    "    \n",
    "    def examine_misclassifications(self):\n",
    "        \"\"\"\n",
    "        Analyze which kinds of songs tend to be misclassified.\n",
    "        This helps us understand the model's blind spots.\n",
    "        \"\"\"\n",
    "        predictions = self.model.predict(self.X)\n",
    "        probabilities = self.model.predict_proba(self.X)\n",
    "        \n",
    "        # Find misclassified instances\n",
    "        misclassified = self.X[predictions != self.y]\n",
    "        misclassified_true = self.y[predictions != self.y]\n",
    "        misclassified_pred = predictions[predictions != self.y]\n",
    "        misclassified_probs = probabilities[predictions != self.y]\n",
    "        \n",
    "        print(\"Analysis of Misclassifications:\")\n",
    "        print(f\"Total misclassified instances: {len(misclassified)}\")\n",
    "        \n",
    "        # Analyze feature patterns in misclassifications\n",
    "        for i, feature in enumerate(self.feature_names):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(self.X[predictions == self.y][:, i], alpha=0.5, \n",
    "                    label='Correct predictions')\n",
    "            plt.hist(misclassified[:, i], alpha=0.5, label='Misclassifications')\n",
    "            plt.title(f'{feature} Distribution in Correct vs Incorrect Predictions')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        return misclassified, misclassified_true, misclassified_pred, misclassified_probs\n",
    "    \n",
    "    def plot_learning_curve(self):\n",
    "        \"\"\"\n",
    "        Generate learning curves to understand how model performance\n",
    "        changes with more training data.\n",
    "        \"\"\"\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            self.model, self.X, self.y, cv=5,\n",
    "            train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "        \n",
    "        # Calculate mean and std\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        test_mean = np.mean(test_scores, axis=1)\n",
    "        test_std = np.std(test_scores, axis=1)\n",
    "        \n",
    "        # Plot learning curve\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_sizes, train_mean, label='Training score')\n",
    "        plt.plot(train_sizes, test_mean, label='Cross-validation score')\n",
    "        plt.fill_between(train_sizes, train_mean - train_std,\n",
    "                        train_mean + train_std, alpha=0.1)\n",
    "        plt.fill_between(train_sizes, test_mean - test_std,\n",
    "                        test_mean + test_std, alpha=0.1)\n",
    "        plt.xlabel('Training Examples')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Learning Curves')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Let's use our evaluation toolkit\n",
    "evaluator = ModelEvaluator(classifier, X, y, preprocessor.feature_names)\n",
    "\n",
    "print(\"Starting comprehensive model evaluation...\")\n",
    "\n",
    "# Analyze feature importance\n",
    "print(\"\\n1. Feature Importance Analysis\")\n",
    "feature_importance = evaluator.analyze_feature_importance()\n",
    "print(\"\\nMost important features for classification:\")\n",
    "for feature, score in feature_importance[:3]:\n",
    "    print(f\"{feature}: {score:.3f}\")\n",
    "\n",
    "# Examine decision boundaries for top two features\n",
    "print(\"\\n2. Decision Boundary Analysis\")\n",
    "evaluator.analyze_decision_boundaries(\n",
    "    feature_importance[0][0],\n",
    "    feature_importance[1][0]\n",
    ")\n",
    "\n",
    "# Analyze misclassifications\n",
    "print(\"\\n3. Misclassification Analysis\")\n",
    "misclassified = evaluator.examine_misclassifications()\n",
    "\n",
    "# Generate learning curves\n",
    "print(\"\\n4. Learning Curve Analysis\")\n",
    "evaluator.plot_learning_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c0084-1577-46c7-b07c-e204a3b67477",
   "metadata": {},
   "source": [
    "Let's break down what we've learned from our evaluation:\n",
    "\n",
    "1. **Feature Importance**: By measuring the separation between class distributions, we can identify which musical features are most crucial for predicting popularity. This helps us understand what makes a song popular according to our model.\n",
    "\n",
    "2. **Decision Boundaries**: By visualizing how our model divides the feature space, we can understand where it's confident in its predictions and where it's less certain. This is particularly valuable for understanding the \"naive\" independence assumption's impact.\n",
    "\n",
    "3. **Misclassification Analysis**: By examining the songs our model gets wrong, we can identify patterns and potential improvements. For example, if we find that high-energy songs are often misclassified, we might need to adjust our feature preprocessing for energy-related features.\n",
    "\n",
    "4. **Learning Curves**: These show us whether we would benefit from more training data or if we're already at the model's performance ceiling. They also help us identify if we're overfitting or underfitting.\n",
    "\n",
    "The insights from this evaluation help us in two ways:\n",
    "- They give us confidence about when we can trust our model's predictions\n",
    "- They point us toward specific improvements we can make\n",
    "\n",
    "In our next section, we'll use these insights to improve our model's performance through various techniques like feature selection and parameter tuning. We'll see how understanding our model's behavior guides us toward effective improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485c0f3-89df-4aa9-a38f-88ef73499502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
